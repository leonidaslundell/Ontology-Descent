---
title: "Benchmarking various ontological packages, distances and clustering method "
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '../')
```

# Introduction & aims

There are three distinct components in this project (existing methods): 

1. Ontological distance metric (Graphd distance, Wang, Resnik etc).
2. Clustering component.
3. Overall, final result (package component, eg Revigo).

GO input terms benchmarking should be consider size (ranging from large to small) and diversity (similar go terms vs dissimilar go terms). Another consideration (further down the line) is clustering behavior with MF & BP & CC combined or separate. 

One big issue with test data is ground truth data. I might have come up with a solution for generating that...

Outline of approach: 

* Validate that the synthetic data look sufficiently diverse.
* Compare public data to the synthetic data
* Figure out appropriate metric for method comparison of synthetic data
* Use public data of to perform camera enrichment and visually inspect the results comparing our method to simplify enrichment.

Quick note: simplify enrichment is not the gold standard. It is a wrapper for several distance and clustering metrics, and it also presents a novel clustering method. I initially only test their novel method, but further down the line i want to benchmark all distance, and all clustering methods against our method. 

# Data

simplifyEnrichment package has a random GO generator with size. The fact that you get clusters from random data is problematic. I **might** have developed a meaningfull GO tree sampler.

For real data i will use the ~30 microarray experiments from the KEGG topological pathway analaysis tools.

## Introduction to data generation

Briefly, generating a clustered ontology dataset is based using the fact that GO terms are ontological. Take a cutree at a sampled height (or rather k), take a sample from the branch, and you have a cluster. This will give you both broad and specific clusters (as you end up cutting randomly). You also known that a certain bunch of ontology terms are related as they have originated from the same tree. This approach **does not** preclude though that different trees are related as you might cut from the same tree upstream or parallel.

One major issue with using this data as ground truth is that its hard to link up the resulting cluster with the originating cluster: ie. you get 50 predicted clusters, which one of all (and perhaps which many of all) are linking to the any one of the test data clusters? My mediocre solution to this is to take a *best fit* approach where you match the generated cluster with the originating clusters based on jackard index (intersect/union). A jackard index of 1 means a perfect fit.

Another major consideration: am i shooting an arrow on the wall and painting a bullseye after the fact? Is the tree data i generate a realistic representaion of reality?

## Generating tree data

```{r, message=FALSE}
source("../Ontology-Descent/R/clustering_functions.R")
library(org.Hs.eg.db)
library(simplifyEnrichment)
library(igraph)
library(ggplot2)
library(data.table)
library(patchwork)
```


```{r}
# net <- networkeR(ont = "MF")
# 
# distNet <- distances(net)
# clustNet <- hclust(as.dist(distNet))
# 
# GOlength <- sapply(as.list(org.Hs.egGO2ALLEGS), length)
# GOlength <- c(GOlength, sapply(as.list(org.Mm.egGO2ALLEGS), length))
# GOnames <- as.list(GO.db::GOTERM)
# GOnames <- sapply(GOnames, Term)
# 
# save(list = c("net", "distNet", "clustNet", "GOlength", "GOnames", "GOnames"), 
#      file = "data/dataClustereR.Rdata")
load("data/dataClustereR.Rdata")
```

## Generating 1000 samples of synthetic IC data

```{r}
# library(parallel)
# 
# clustNet2 <- GO_similarity(V(net)$name, ont = "MF")
# clustNet2 <- hclust(as.dist(clustNet2))
# 
# datRel <- mclapply(1:1000, function(n){
#   # print(n)
#   sampleR(ontoHclust = clustNet2, n = sample(5:50,1), maxClusterSize = 250, minClusters = 15, maxClusters = 1000)
# }, mc.cores = 16)
# datRel <- datRel[sapply(datRel, length)>0]
# hist(sapply(datRel, function(x) length(unique(unlist(x)))))
# beepr::beep()
# save(datRel, file = "data/1000sampleRrel.Rdata")

load("data/1000sampleRrel.Rdata")

```

## Generating 1000 samples of synthetic distance data

```{r}
# library(parallel)
# datClust <- mclapply(1:1000, function(n){
#   # print(n)
#   sampleR(ontoHclust = clustNet, n = sample(5:50,1))
# }, mc.cores = 16)
# datClust <- datClust[sapply(datClust, length)>0]
# save(datClust, file = "data/1000sampleRdistance.Rdata")

load("data/1000sampleRdistance.Rdata")
```

Have not investigated the effect of number of k taken for sampleR function. Current implementation is:
sample(20:1000,1), but maybe we can make more realistic data with different parameters?

## Generating real GO data

 Camera analysis for GO terms

```{r}
# library(limma)
# library(KEGGandMetacoreDzPathwaysGEO)
# library(KEGGdzPathwaysGEO)
# library(Biobase)
# library(hgu133plus2.db)
# library(hgu133a.db)
# library(annotate)
# 
# data("GSE1145")
# 
# plus2 <- as.list(hgu133plus2ENTREZID)
# sum(!rownames(GSE1145) == names(plus2))
# names(plus2)[!rownames(GSE1145) == names(plus2)]
# #some afymetrix specific stuff.
# 
# data("GSE1297")
# a <- as.list(hgu133aENTREZID)
# sum(!rownames(GSE1297) == names(a))
# names(a)[!rownames(GSE1297) == names(a)]
# 
# x <- as.list(org.Hs.egGO2ALLEGS)
# plus2 <- ids2indices(gene.sets = x,identifiers = unlist(plus2))
# a <- ids2indices(gene.sets = x,identifiers = unlist(a))
# 
# mysets <- c(data(package="KEGGandMetacoreDzPathwaysGEO")$results[,"Item"],
#             data(package="KEGGdzPathwaysGEO")$results[,"Item"])
# 
# datGSE <- NULL
# 
# for(gse in mysets){
#   print(gse)
# 
#   eval(parse(text = paste0("data(", gse, ")")))
#   eval(parse(text = paste0("dat <- ", gse)))
#   eval(parse(text = paste0("rm(", gse, ")")))
#   
#   datGSE[[gse]] <- "NA"
#   
#   if(annotation(dat) == "hgu133plus2")
#     datGSE[[gse]] <- camera(exprs(dat), plus2, model.matrix(~0 + Group, dat))
# 
#   if(annotation(dat) == "hgu133a")
#     datGSE[[gse]] <- camera(exprs(dat), a, model.matrix(~0 + Group, dat))
# 
# }
# datGSE <- datGSE
# save(datGSE, file = "data/PublicGSEdata_GOcamera.Rdata")
load("data/PublicGSEdata_GOcamera.Rdata")
```

## Generating 1000 completely random GO sets with variable size

```{r}
# x <- sapply(datGSE, length)
# # x <- rnorm(1000, mean = mean(x), sd = sd(x))
# x <- rpois(1000, x)
# 
# datRand <- mclapply(x, function(size){
#   random_GO(n = size, ont = "MF")
# })
# save(datRand, file = "data/1000sampleRandom.Rdata")
load("data/1000sampleRandom.Rdata")
```

## Comparing the shape and properties of the real GO data and the synthetic data

### Summary statistics of real GO terms

```{r}
datGSE <- lapply(datGSE, function(x){
  x <- rownames(x[x$FDR<0.05,])
  x[x %in% V(net)$name] #keeping only MF
})
```

Remove maps without any results.

```{r}
datGSE <- datGSE[sapply(datGSE, length)>1]
```

### Distances of GO - GO terms

```{r}
resGSEDist <- lapply(datGSE, function(x){
  temp <- distances(net, x, x)
  temp <- reshape2::melt(temp[upper.tri(temp)])
  temp$value
})

resGSEDist <- reshape2::melt(resGSEDist)
resGSEDist <- as.data.table(resGSEDist)

resClustDist <- lapply(datClust[sample(1:length(datClust), 200, replace = F)], function(x){
  x <- unique(unlist(x))
  temp <- distances(net, x, x)
  temp <- reshape2::melt(temp[upper.tri(temp)])
  temp$value
})
resClustDist <- reshape2::melt(resClustDist)
resClustDist <- as.data.table(resClustDist)

resRandDist <- lapply(datRand[sample(1:length(datRand), 200, replace = F)], function(x){
  x <- unique(unlist(x))
  temp <- distances(net, x, x)
  temp <- reshape2::melt(temp[upper.tri(temp)])
  temp$value
})
resRandDist <- reshape2::melt(resRandDist)
resRandDist <- as.data.table(resRandDist)

resRelDist <- lapply(datRel[sample(1:length(datRel), 200, replace = F)], function(x){
  x <- unique(unlist(x))
  temp <- distances(net, x, x)
  temp <- reshape2::melt(temp[upper.tri(temp)])
  temp$value
})
resRelDist <- reshape2::melt(resRelDist)
resRelDist <- as.data.table(resRelDist)
```

This is actually quite a difference at 9 (ie synthetic data seems to have a slightly larger distance between its terms?)

### Are the graph parameters different between the synthetic and the real data?

```{r}
#warning suppresed is that closseness is not well defined for disconnected graphs

suppressWarnings({
  
  resGSEParameters <- sapply(datGSE, function(n){
    indNet <- induced_subgraph(net, vids = unique(unlist(n)))
    c(degree = centr_degree(indNet)$centralization,
      betweenness = centr_betw(indNet)$centralization,
      closeness = centr_clo(indNet)$centralization,
      eigen = centr_eigen(indNet)$centralization,
      componenets = count_components(indNet),
      diameters = diameter(indNet))
  })
  
  
  
  resClustParameters <- sapply(datClust, function(n){
    indNet <- induced_subgraph(net, vids = unique(unlist(n)))
    c(degree = centr_degree(indNet)$centralization,
      betweenness = centr_betw(indNet)$centralization,
      closeness = centr_clo(indNet)$centralization,
      eigen = centr_eigen(indNet)$centralization,
      componenets = count_components(indNet),
      diameters = diameter(indNet))
  })
  
  resRelParameters <- sapply(datRel, function(n){
    indNet <- induced_subgraph(net, vids = unique(unlist(n)))
    c(degree = centr_degree(indNet)$centralization,
      betweenness = centr_betw(indNet)$centralization,
      closeness = centr_clo(indNet)$centralization,
      eigen = centr_eigen(indNet)$centralization,
      componenets = count_components(indNet),
      diameters = diameter(indNet))
  })

  resRandParameters <- sapply(datRand, function(n){
    indNet <- induced_subgraph(net, vids = unique(unlist(n)))
    c(degree = centr_degree(indNet)$centralization,
      betweenness = centr_betw(indNet)$centralization,
      closeness = centr_clo(indNet)$centralization,
      eigen = centr_eigen(indNet)$centralization,
      componenets = count_components(indNet),
      diameters = diameter(indNet))
  })
})

resGSEParameters <- reshape2::melt(resGSEParameters)
resGSEParameters$Var2 <- as.numeric(resGSEParameters$Var2)

resClustParameters <- reshape2::melt(resClustParameters)
resRandParameters <- reshape2::melt(resRandParameters)
resRelParameters <- reshape2::melt(resRelParameters)

allParameters <- rbind(cbind(resGSEParameters, data = "Public\nReal"),
                       cbind(resClustParameters, data = "Synthetic\nCluster"),
                       cbind(resRandParameters, data = "Synthetic\nRandom"),
                       cbind(resRelParameters, data = "Synthetic\nRel"))
allParameters <- as.data.table(allParameters)
```

### Comparing the synthetic data and real data structures and large scale properties

```{r}
#number of unique terms
par(mfrow=c(4,1))
hist(sapply(datGSE, function(GO) length(GO)), main = "Real Public data", xlab = "Number of GO terms", xlim = c(0, 1200))
hist(sapply(datClust, function(GO) length(unique(unlist(GO)))), main = "Synthetic Clustered data", xlab = "Number of GO terms", xlim = c(0, 1200))
hist(sapply(datRel, function(GO) length(unique(unlist(GO)))), main = "Synthetic Rel data", xlab = "Number of GO terms", xlim = c(0, 1200))
hist(sapply(datRand, function(GO) length(unique(unlist(GO)))), main = "Synthetic Random data", xlab = "Number of GO terms", xlim = c(0, 1200))

#distance of all to all

distAlltoAll <- rbind(cbind(resGSEDist, data = "Public Real data"),
                    cbind(resClustDist, data = "Synthetic Cluster data"),
                    cbind(resRelDist, data = "Synthetic Rel data"),
                    cbind(resRandDist, data = "Synthetic Random data"))

ggplot(distAlltoAll[,median(value), by = c("L1", "data")], 
       aes(x = V1, color = data, group = data)) +
  geom_density(show.legend = T) +
  xlab("Distance of all to all in sample") +
  ggtitle("Median") +
ggplot(distAlltoAll[,max(value), by = c("L1", "data")], 
       aes(x = V1, color = data, group = data)) +
  geom_density(show.legend = T) +
  xlab("Distance of all to all in sample") +
  ggtitle("Max") +
  plot_layout(guides = "collect")
ggsave("ClusteringAlgoDevelopment/figures/network.distAllToAll.pdf", width = 8, height = 5)
#graph structure

ggplot(allParameters, aes(x = value, group = Var2, fill = data)) +
  geom_histogram() +
  facet_grid(data~Var1, scales = "free") +
  theme(legend.position = "none")
ggsave("ClusteringAlgoDevelopment/figures/network.parameters.pdf", width = 10, height = 5)
```

What do the different graph measures mean? 

* Degree score for a node - simplest score, how many nodes do you connect to.
* Betweenness score for a node - how many shortest paths between all pairs of vertices pass through a given vertex.
* Closenes score for a node - 1/sum(length to all vertices from a node). Calculates how close you are to everything.
* Eigenvector score for a node - linear algebra mathemagic that tells you whether a node connects to other well connected nodes. Gangster nodes basically.
* Diameter of a graph - what is the biggest shortest path in the graph
* Components of a graph - how many isolated islands does a graph have. Counts single isolated vertices as well.

And a nice wikipedia figure:

```{r, echo=FALSE, fig.cap="Examples of A) Betweenness centrality, B) Closeness centrality, C) Eigenvector centrality, D) Degree centrality, E) Harmonic centrality and F) Katz centrality of the same graph", out.width = '100%'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/1/11/6_centrality_measures.png")
```

# Comparing network clustering to classical methods

Intend to Rand index meassures accuracy for clustering results. Very interesting statistical [algorithm](https://en.wikipedia.org/wiki/Rand_index). The rand index is a specialized form of **accuracy**

The binary cut algorithm seems to work very well. Its main drawback is that *it doesnt always work*. Suspiciously, it doesnt seem to fail with random data, only synthetic clustered and real data?!?

```{r}
# sapply(resRand[1:50], function(x){
#   R.utils::withTimeout(simplifyGO(GO_similarity(unique(unlist(x), ont = "MF")), plot = F), timeout = 120, onTimeout= "error")
# })
# sapply(datGSE, function(x){
#   R.utils::withTimeout(simplifyGO(GO_similarity(unique(unlist(x), ont = "MF")), plot = F), timeout = 1, onTimeout= "warning")
# })
# #does not progress past 120 on first (happens on many other)
# sapply(datClust, function(x){
#   R.utils::withTimeout(simplifyGO(GO_similarity(unique(unlist(x), ont = "MF")), plot = F), timeout = 120, onTimeout= "error")
# })
# #does not progress past 120 on 8th

```

## Running tests on both synthetic datasets

Applying the algorithms on the synthetic data Commented-out since it takes long...

<!-- FIXED THIS WITH THE cluster_terms internal catch function -->
<!-- NOTE THAT binary cut still ends up in some kind of endless loop -->
<!-- First, kmeans crashes with fewer than ~15 terms. also its unintereinst... -->
<!-- ```{r} -->
<!-- datClust <- datClust[sapply(datClust, function(i){length(unique(unlist(i)))})>20] -->
<!-- datRel <- datRel[sapply(datRel, function(i){length(unique(unlist(i)))})>20] -->
<!-- ``` -->

```{r}
library(igraph)
library(simplifyEnrichment)
library(data.table)

resClust <- lapply(datClust[1:100], function(truth){

  targets <- unique(unlist(truth))
  test <- clustereR(net, GOnames, GOlength, targets)$res
  test <- structure(test$cluster, names = test$ontoID)
  test
})
save(file = "data/resultsGroundTruthClustereR.rdata", resClust)

resClassic <- NULL
for(meth in all_clustering_methods()[-c(1,11)][1:2]){
  print(meth)
  
  #datclust 563 crashes. not length dependent? other datclust are equally long
  resClassic[[meth]] <- lapply(datClust[1:100], function(truth){
    targets <- unique(unlist(truth))
    test <- cluster_terms(GO_similarity(targets, "MF"), method = meth, catch_error = T)
    test <- try(structure(as.numeric(test), names = targets))
    test
  })
}
save(file = "data/resultsGroundTruthOtherMethods.rdata", resClassic)

load("data/resultsGroundTruthOtherMethods.rdata")
load("data/resultsGroundTruthClustereR.rdata")
```

Test also with the synthetic rel data.

```{r}
resClustRel <- lapply(datRel[1:100], function(truth){
  targets <- unique(unlist(truth))
  test <- clustereR(net, GOnames, GOlength, targets)$res
  test <- structure(test$cluster, names = test$ontoID)
  test
})
save(file = "data/resultsGroundTruthClustereRrel.rdata", resClustRel)

resClassicRel <- NULL
for(meth in all_clustering_methods()[-c(1,11)][1:2]){
  print(meth)

  resClassicRel[[meth]] <- lapply(datRel[1:100], function(truth){
    targets <- unique(unlist(truth))
    test <- cluster_terms(GO_similarity(targets, "MF"), method = meth, catch_error = T)
    test <- try(structure(as.numeric(test), names = targets))
    test
  })
}
save(file = "data/resultsGroundTruthOtherMethodsRel.rdata", resClassicRel)

load("data/resultsGroundTruthClustereRrel.rdata")
load("data/resultsGroundTruthOtherMethodsRel.rdata")
```

## Comparing the results

Melt grund truth and resorder results have same order as ground truth

```{r}

for(i in 1:length(datClust)){
  x <- reshape2::melt(datClust[[i]])
  datClust[[i]] <- structure(x$L1, names = x$value)
}

for(i in 1:length(resClust)){
  resClust[[i]] <- resClust[[i]][names(datClust[[i]])]
}

for(meth in names(resClassic)){
  for(i in 1:length(resClassic[[meth]])){
    resClassic[[meth]][[i]] <- resClassic[[meth]][[i]][names(datClust[[i]])]
  }
}

# same for the rel data

for(i in 1:length(datRel)){
  x <- reshape2::melt(datRel[[i]])
  datRel[[i]] <- structure(x$L1, names = x$value)
}

for(i in 1:length(resClustRel)){
  resClustRel[[i]] <- resClustRel[[i]][names(datRel[[i]])]
}

for(meth in names(resClassicRel)){
  for(i in 1:length(resClassicRel[[meth]])){
    resClassicRel[[meth]][[i]] <- resClassicRel[[meth]][[i]][names(datRel[[i]])]
  }
}


```

Analyze using fossils rand.index.

```{r}
library(fossil)

resComparisons <- as.data.frame(matrix(NA, 
                                       nrow = length(resClassic$kmeans),
                                       ncol = length(resClassic)+1))
colnames(resComparisons) <- c("clustereR", names(resClassic))

for(i in 1:length(resClassic$kmeans)){
 resComparisons[i, "clustereR"] <- adj.rand.index(datClust[[i]], resClust[[i]])
 resComparisons[i, "kmeans"] <- adj.rand.index(datClust[[i]], resClassic$kmeans[[i]])
 resComparisons[i, "dynamicTreeCut"] <- adj.rand.index(datClust[[i]], resClassic$dynamicTreeCut[[i]])
}

#and for rel

resComparisonsRel <- as.data.frame(matrix(NA, 
                                       nrow = length(resClassicRel$kmeans),
                                       ncol = length(resClassicRel)+1))
colnames(resComparisonsRel) <- c("clustereR", names(resClassicRel))

for(i in 1:length(resClassicRel$kmeans)){
 resComparisonsRel[i, "clustereR"] <- adj.rand.index(datRel[[i]], resClustRel[[i]])
 resComparisonsRel[i, "kmeans"] <- adj.rand.index(datRel[[i]], resClassicRel$kmeans[[i]])
 resComparisonsRel[i, "dynamicTreeCut"] <- adj.rand.index(datRel[[i]], resClassicRel$dynamicTreeCut[[i]])
}
```

Plot

```{r}
ggplot(reshape2::melt(resComparisons, variable.name = "method"), aes(x = value, color = method)) +
  stat_ecdf() + ggtitle("Rand index for distance based synthetic data") + 
  ylab("Proportion") +
  xlab("Rand index") +
ggplot(reshape2::melt(resComparisonsRel, variable.name = "method"), aes(x = value, color = method)) +
  stat_ecdf() + ggtitle("Rand index for Rel based synthetic data") +  
  ylab("Proportion") +
  xlab("Rand index") +
  plot_layout(guides = "collect", nrow = 2) &
  theme(legend.position = "bottom")

ggsave("ClusteringAlgoDevelopment/figures/accuracy.pdf", width = 5, height = 6)

```

Is the accuracy dependent on size? Think Rel have slighty different graph properties than the other methods.

```{r}
ggplot(reshape2::melt(cbind(resComparisons, length = sapply(resClust[1:100], length)), id.vars = "length"), 
       aes(x = length, y = value, color = variable, group = variable)) +
  geom_point() +
  scale_y_log10() +
  geom_smooth(se = F, method = "lm") +
  ylab("Rand index (log10)") + xlab("Number of GO terms") + ggtitle("Rand index for distance based synthetic data") +
  
ggplot(reshape2::melt(cbind(resComparisonsRel, length = sapply(resClust[1:100], length)), id.vars = "length"), 
       aes(x = length, y = value, color = variable, group = variable)) +
  geom_point() +
  scale_y_log10() +
  geom_smooth(se = F, method = "lm") +
  ylab("Rand index (log10)") + xlab("Number of GO terms") + ggtitle("Rand index for Rel based synthetic data") +
  plot_layout(guides = "collect", nrow = 2) &
  theme(legend.position = "bottom")
ggsave("ClusteringAlgoDevelopment/figures/numberGOterms.randIndex.pdf", width = 5, height = 6)
```

Is the Rel data clustered? Lets inspect.

```{r}
plotClust <- function(targets, tit = NULL){
  localNet <- shortest_paths(net, from = names(targets), to = names(targets), mode = "all")
  localNet <- localNet$vpath
  localNet <- unique(c(names(targets), names(unlist(localNet))))
  localNet <- igraph::induced_subgraph(net, localNet)
  
  V(localNet)$name
  
  #color by generated cluster
  cols <- colorRampPalette(RColorBrewer::brewer.pal(12, "Set3"))(max(targets))
  # cols <- sample(cols, max(targets), replace = F)
  cols <- cols[targets]
  names(cols) <- names(targets)
  
  V(localNet)$frame.width <- 0.0001
  V(localNet)$color <- "gray"
  # all(V(localNet)$name[match(names(cols), V(localNet)$name)] == names(cols))
  V(localNet)$color[match(names(cols), V(localNet)$name)] <- cols
  V(localNet)$size[V(localNet)$color == "gray"] <- 1
  V(localNet)$size[V(localNet)$color != "gray"] <- 5
  
  E(localNet)$arrow.size <- .1
  return(plot(localNet, vertex.label = NA, main = tit))
}


pdf("ClusteringAlgoDevelopment/figures/clusters.rel.vs.distance.pdf", width = 6, height = 6)

plotClust(datRel[[4]], tit = "rel 1")
plotClust(datRel[[5]], tit = "rel 2")
plotClust(datRel[[6]], tit = "rel 3")
plotClust(datRel[[7]], tit = "rel 4")

plotClust(datClust[[4]], tit = "dist 1")
plotClust(datClust[[7]], tit = "dist 2")
plotClust(datClust[[8]], tit = "dist 3")
plotClust(datClust[[9]], tit = "dist 4")

dev.off()
```

There seems to be something fundamentaly wrong with the IC based clustering...

```{r}
set.seed(42)
targets <- sample(V(net)$name, 1000)

x <- lapply(c("Wang", "Resnik", "Rel", "Jiang", "Lin"), function(i){
  GO_similarity(targets, ont = "MF", measure = i)
})

names(x) <- c("Wang", "Resnik", "Rel", "Jiang", "Lin")

localNet <- shortest_paths(net, from = targets, to = targets, mode = "all")
localNet <- localNet$vpath
localNet <- unique(c(targets, names(unlist(localNet))))
localNet <- igraph::induced_subgraph(net, localNet)

x$topologicalDist <- distances(localNet)
x$topologicalDist <- x$topologicalDist[targets, targets]

pdf("ClusteringAlgoDevelopment/figures/dendro.IC.methods.pdf", 5,7)
par(mfrow = c(3,2))
sapply(names(x), function(i){
  library(dendextend)
  print({
    y <- as.dendrogram(hclust(as.dist(x[[i]])))
    y <- dendextend::color_branches(y,k=6)
    suppressWarnings(plot(y %>% set("labels", NULL), main = i, labels = F))
  })
})
dev.off()

x <- lapply(x, function(i){
  cutree(hclust(as.dist(i)), k = 6)
})

pdf("ClusteringAlgoDevelopment/figures/igraph.IC.methods.pdf", 5,7)
par(mfrow = c(3,2), mar = c(1,1,1,1))
sapply(names(x), function(tit){
  set.seed(42)
  print(plotClust(x[[tit]], tit))
})
dev.off()

```

So the IC distance does not comply at all with the hierachical topological clustering...

Lets just visualize the terms in one topological cluster

```{r}
print("Group 5, 16 members")
GOnames[names(x$topologicalDist[x$topologicalDist == 5])]
print("Group 4, 81 members")
GOnames[names(x$topologicalDist[x$topologicalDist == 4])]
print("Group 3, 31 members")
GOnames[names(x$topologicalDist[x$topologicalDist == 3])]
```

Lets just visualize the terms in one of the IC clusters. Cant pick any particular that stands out as better...

```{r}
print("Group 6, 42 members")
GOnames[names(x$Rel[x$Rel == 6])]
print("Group 2, 51 members")
GOnames[names(x$Rel[x$Rel == 2])]
```

Yeah gibberish...


